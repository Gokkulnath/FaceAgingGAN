{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN \n",
    "![](resources/cyclegan_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TLDR:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Assume the following folder structure for creating the Custom Dataset.\n",
    "\n",
    "Dataset_NAME/\n",
    "    - train/\n",
    "        - A/\n",
    "          - img1.jpg\n",
    "          - img2.jpg\n",
    "          - ...\n",
    "        - B/\n",
    "          - img1.jpg\n",
    "          - img2.jpg\n",
    "          - ...\n",
    "    - test/\n",
    "        - A/\n",
    "          - img1.jpg\n",
    "          - img2.jpg\n",
    "          - ...\n",
    "        - B/\n",
    "          - img1.jpg\n",
    "          - img2.jpg\n",
    "          - ...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned # used to handle cases when the number of images are not equal\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)]) # Safe-indexing such that always withing the range\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling of Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # Pad --> Conv2d (Same) --> IntanceNorm2d --> Relu --> Pad --> Conv2d (Same)  --> IntanceNorm2d\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x) # Adds the shortcut/bypass connection\n",
    "\n",
    "#_____________________________________________________#\n",
    "#______________________Generator______________________#\n",
    "#_____________________________________________________#\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks=9):\n",
    "        '''\n",
    "        input_shape : Tensor in (C,H,W) Format\n",
    "        num_residual_blocks : Number of Residual blocks\n",
    "        '''\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model) #Build Sequential model by list unpacking\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "#_____________________________________________________#\n",
    "#__________________Discriminator______________________#\n",
    "#_____________________________________________________#\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self.discriminator_block(channels, 64, normalize=False),\n",
    "            *self.discriminator_block(64, 128),\n",
    "            *self.discriminator_block(128, 256),\n",
    "            *self.discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)), # \n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "    \n",
    "    def discriminator_block(self,in_filters, out_filters, normalize=True):\n",
    "        \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "        layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_filters))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 3, 200, 200]         --\n",
      "|    └─ReflectionPad2d: 2-1                   [-1, 3, 206, 206]         --\n",
      "|    └─Conv2d: 2-2                            [-1, 64, 200, 200]        9,472\n",
      "|    └─InstanceNorm2d: 2-3                    [-1, 64, 200, 200]        --\n",
      "|    └─ReLU: 2-4                              [-1, 64, 200, 200]        --\n",
      "|    └─Conv2d: 2-5                            [-1, 128, 100, 100]       73,856\n",
      "|    └─InstanceNorm2d: 2-6                    [-1, 128, 100, 100]       --\n",
      "|    └─ReLU: 2-7                              [-1, 128, 100, 100]       --\n",
      "|    └─Conv2d: 2-8                            [-1, 256, 50, 50]         295,168\n",
      "|    └─InstanceNorm2d: 2-9                    [-1, 256, 50, 50]         --\n",
      "|    └─ReLU: 2-10                             [-1, 256, 50, 50]         --\n",
      "|    └─ResidualBlock: 2-11                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-1                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-12                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-2                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-13                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-3                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-14                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-4                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-15                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-5                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-16                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-6                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-17                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-7                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-18                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-8                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─ResidualBlock: 2-19                    [-1, 256, 50, 50]         --\n",
      "|    |    └─Sequential: 3-9                   [-1, 256, 50, 50]         1,180,160\n",
      "|    └─Upsample: 2-20                         [-1, 256, 100, 100]       --\n",
      "|    └─Conv2d: 2-21                           [-1, 128, 100, 100]       295,040\n",
      "|    └─InstanceNorm2d: 2-22                   [-1, 128, 100, 100]       --\n",
      "|    └─ReLU: 2-23                             [-1, 128, 100, 100]       --\n",
      "|    └─Upsample: 2-24                         [-1, 128, 200, 200]       --\n",
      "|    └─Conv2d: 2-25                           [-1, 64, 200, 200]        73,792\n",
      "|    └─InstanceNorm2d: 2-26                   [-1, 64, 200, 200]        --\n",
      "|    └─ReLU: 2-27                             [-1, 64, 200, 200]        --\n",
      "|    └─ReflectionPad2d: 2-28                  [-1, 64, 206, 206]        --\n",
      "|    └─Conv2d: 2-29                           [-1, 3, 200, 200]         9,411\n",
      "|    └─Tanh: 2-30                             [-1, 3, 200, 200]         --\n",
      "===============================================================================================\n",
      "Total params: 11,378,179\n",
      "Trainable params: 11,378,179\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 26.11\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 152.28\n",
      "Params size (MB): 43.40\n",
      "Estimated Total Size (MB): 196.14\n",
      "-----------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1, 12, 12]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 100, 100]        3,136\n",
      "|    └─LeakyReLU: 2-2                    [-1, 64, 100, 100]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 128, 50, 50]         131,200\n",
      "|    └─InstanceNorm2d: 2-4               [-1, 128, 50, 50]         --\n",
      "|    └─LeakyReLU: 2-5                    [-1, 128, 50, 50]         --\n",
      "|    └─Conv2d: 2-6                       [-1, 256, 25, 25]         524,544\n",
      "|    └─InstanceNorm2d: 2-7               [-1, 256, 25, 25]         --\n",
      "|    └─LeakyReLU: 2-8                    [-1, 256, 25, 25]         --\n",
      "|    └─Conv2d: 2-9                       [-1, 512, 12, 12]         2,097,664\n",
      "|    └─InstanceNorm2d: 2-10              [-1, 512, 12, 12]         --\n",
      "|    └─LeakyReLU: 2-11                   [-1, 512, 12, 12]         --\n",
      "|    └─ZeroPad2d: 2-12                   [-1, 512, 13, 13]         --\n",
      "|    └─Conv2d: 2-13                      [-1, 1, 12, 12]           8,193\n",
      "==========================================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 992.01\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 9.11\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 20.11\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokkulnath\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchsummary\\layer_info.py:75: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.macs += int(param.nelement() * np.prod(self.output_size[2:]))\n"
     ]
    }
   ],
   "source": [
    "inp_shape = (3,200,200)\n",
    "summary(GeneratorResNet(inp_shape, 9),inp_shape);\n",
    "summary(Discriminator(inp_shape),inp_shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CycleGAN used Last 50 Images to stabilize the trainning \n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    CycleGAN uses Last 50 Images as buffer to stabilize the trainning process\n",
    "    ReplayBuffer is a list that holds the last 50 generated images in a list\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Warning: Trying to create an Empty buffer\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5: # Randomly replace/overwrite existing values\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "epoch=offset=0 \n",
    "\n",
    "decay_start_epoch=3\n",
    "input_shape = (3,40,40)\n",
    "c,img_height,img_width = input_shape\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "checkpoint_interval = 1\n",
    "sample_interval = 100\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,itertools\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Create sample and checkpoint directories\n",
    "dataset_name = 'old2young'\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, num_residual_blocks=3)\n",
    "G_BA = GeneratorResNet(input_shape, num_residual_blocks=3)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)\n",
    "\n",
    "gpu_flag = torch.cuda.is_available()\n",
    "\n",
    "if gpu_flag:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()\n",
    "\n",
    "if epoch != 0: # Resuming Trainning from Checkpoints\n",
    "    # Load pretrained models\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "decay_lr_step = lambda epoch : 1.0 - max(0,epoch + offset - decay_start_epoch)/(n_epochs - decay_start_epoch)\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=decay_lr_step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=decay_lr_step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=decay_lr_step)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if gpu_flag else torch.Tensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Image transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "train_dataloader = DataLoader(\n",
    "    CycleGANDataset(dataset_name, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    CycleGANDataset(dataset_name, transforms_=transforms_, unaligned=True, mode=\"test\"),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning loop and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning Processs:\n",
    "    Train Generators (A and B) --> Discriminator A --> Discrminator B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch : 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, n_epochs):\n",
    "    print(f\"Starting Epoch : {epoch}\")\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # Set model input\n",
    "        real_A = batch[\"A\"].to(device)\n",
    "        real_B = batch[\"B\"].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(((real_A.size(0), *D_A.output_shape)), requires_grad=False).to(device)\n",
    "        fake = torch.zeros(((real_A.size(0), *D_A.output_shape)), requires_grad=False).to(device)\n",
    "        \n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_A = G_BA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator A\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator B\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "        # Print log\n",
    "        batches_done = epoch * len(train_dataloader) + i\n",
    "\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f]\"\n",
    "            % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(train_dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_cycle.item(),\n",
    "                loss_identity.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
